{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORf1UgwMMZ4uAt6tnLhO11",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LahariSivalasetty/newone/blob/customer_churn/customer_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: prompt: using random forest algorithm generate code for customer churn prediction for predictive analysis and genearte dattasets by importing for large datasewt'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# To use a large dataset, you would typically load it from a file (e.g., CSV).\n",
        "# For demonstration purposes, let's assume you have a CSV file named 'churn_dataset.csv'\n",
        "# and modify the code to load from it.\n",
        "\n",
        "# First, you might need to upload your file to Google Colab if it's on your local machine\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# Or if the file is in your Google Drive, you can mount your Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# And then load from the Drive path, e.g., pd.read_csv('/content/drive/My Drive/churn_dataset.csv')\n",
        "\n",
        "# For this example, let's keep the synthetic data generation part for reproducibility,\n",
        "# but note that in a real scenario, you would replace this with loading your large dataset.\n",
        "# Let's increase the number of samples to simulate a larger dataset within the synthetic generation.\n",
        "\n",
        "# Generate a larger synthetic dataset\n",
        "np.random.seed(42)\n",
        "n_samples_large = 10000  # Increased number of samples to simulate a larger dataset\n",
        "data_large = {\n",
        "    'CustomerID': range(1, n_samples_large + 1),\n",
        "    'Age': np.random.randint(18, 70, size=n_samples_large),\n",
        "    'Gender': np.random.choice(['Male', 'Female'], size=n_samples_large, p=[0.55, 0.45]),\n",
        "    'Tenure': np.random.randint(0, 72, size=n_samples_large), # Increased max tenure\n",
        "    'MonthlyCharges': np.random.uniform(15, 120, size=n_samples_large).round(2), # Slightly wider range\n",
        "    'TotalCharges': np.random.uniform(50, 8000, size=n_samples_large).round(2), # Wider range\n",
        "    'ContractType': np.random.choice(['Month-to-month', 'One year', 'Two year'], size=n_samples_large, p=[0.55, 0.25, 0.20]), # Adjusted probabilities\n",
        "    'InternetService': np.random.choice(['DSL', 'Fiber optic', 'No'], size=n_samples_large, p=[0.25, 0.50, 0.25]), # Fiber optic more common\n",
        "    'PaymentMethod': np.random.choice(['Electronic check', 'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)'], size=n_samples_large),\n",
        "    'Churn': np.random.choice([0, 1], size=n_samples_large, p=[0.7, 0.3]) # Target variable (slightly higher churn rate)\n",
        "}\n",
        "\n",
        "df_large = pd.DataFrame(data_large)\n",
        "\n",
        "# Introduce some complexity/patterns (same logic, applied to larger data)\n",
        "df_large.loc[df_large['Tenure'] < 18, 'Churn'] = np.random.choice([0, 1], size=df_large.loc[df_large['Tenure'] < 18].shape[0], p=[0.45, 0.55])\n",
        "df_large.loc[(df_large['MonthlyCharges'] > 90) & (df_large['ContractType'] == 'Month-to-month'), 'Churn'] = np.random.choice([0, 1], size=df_large.loc[(df_large['MonthlyCharges'] > 90) & (df_large['ContractType'] == 'Month-to-month')].shape[0], p=[0.35, 0.65])\n",
        "df_large.loc[df_large['InternetService'] == 'Fiber optic', 'Churn'] = np.random.choice([0, 1], size=df_large.loc[df_large['InternetService'] == 'Fiber optic'].shape[0], p=[0.6, 0.4])\n",
        "# Add a new pattern: higher age might correlate with lower churn\n",
        "df_large.loc[df_large['Age'] > 55, 'Churn'] = np.random.choice([0, 1], size=df_large.loc[df_large['Age'] > 55].shape[0], p=[0.85, 0.15])\n",
        "\n",
        "\n",
        "print(f\"Generated dataset with {len(df_large)} rows.\")\n",
        "print(\"Dataset head:\")\n",
        "print(df_large.head())\n",
        "print(\"\\nChurn Distribution:\")\n",
        "print(df_large['Churn'].value_counts(normalize=True))\n",
        "\n",
        "\n",
        "# Data Preprocessing\n",
        "# Handle categorical features using one-hot encoding\n",
        "# Drop CustomerID as it's not a feature\n",
        "df_large_encoded = pd.get_dummies(df_large.drop('CustomerID', axis=1),\n",
        "                                  columns=['Gender', 'ContractType', 'InternetService', 'PaymentMethod'],\n",
        "                                  drop_first=True) # drop_first=True avoids multicollinearity\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X_large = df_large_encoded.drop('Churn', axis=1)\n",
        "y_large = df_large_encoded['Churn']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# Using a standard split, stratifying to maintain churn ratio in both sets\n",
        "X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
        "    X_large, y_large, test_size=0.25, random_state=42, stratify=y_large) # Using 25% for testing\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train_large)}\")\n",
        "print(f\"Testing set size: {len(X_test_large)}\")\n",
        "\n",
        "# Model Training - Random Forest\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "# Increased n_estimators for potentially better performance on a larger dataset,\n",
        "# but be mindful of computation time.\n",
        "# Using 'balanced_subsample' might be better than 'balanced' for very large datasets,\n",
        "# but 'balanced' is generally fine for datasets up to tens of thousands.\n",
        "rf_model_large = RandomForestClassifier(n_estimators=200, random_state=42,\n",
        "                                        class_weight='balanced', n_jobs=-1) # n_jobs=-1 uses all available cores\n",
        "\n",
        "print(\"\\nTraining Random Forest model...\")\n",
        "# Train the model on the training data\n",
        "rf_model_large.fit(X_train_large, y_train_large)\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Model Evaluation\n",
        "\n",
        "print(\"\\nEvaluating model performance...\")\n",
        "# Make predictions on the test set\n",
        "y_pred_large = rf_model_large.predict(X_test_large)\n",
        "y_proba_large = rf_model_large.predict_proba(X_test_large)[:, 1] # Probability of churning\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_large = accuracy_score(y_test_large, y_pred_large)\n",
        "report_large = classification_report(y_test_large, y_pred_large)\n",
        "confusion_large = confusion_matrix(y_test_large, y_pred_large)\n",
        "roc_auc_large = roc_auc_score(y_test_large, y_proba_large)\n",
        "\n",
        "print(\"Model Evaluation on Large Dataset:\")\n",
        "print(f\"Accuracy: {accuracy_large:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report_large)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_large)\n",
        "print(f\"\\nROC AUC Score: {roc_auc_large:.4f}\")\n",
        "\n",
        "# Feature Importance (Optional but insightful)\n",
        "importances_large = rf_model_large.feature_importances_\n",
        "feature_names_large = X_large.columns\n",
        "feature_importance_df_large = pd.DataFrame({'feature': feature_names_large, 'importance': importances_large})\n",
        "feature_importance_df_large = feature_importance_df_large.sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance (Top 10):\")\n",
        "print(feature_importance_df_large.head(10)) # Print top 10 features\n",
        "\n",
        "# Making Predictions on New Data (Example using the trained large model)\n",
        "# Suppose you have a new customer's data in a DataFrame\n",
        "new_customer_data_example = pd.DataFrame({\n",
        "    'Age': [28],\n",
        "    'Gender': ['Male'],\n",
        "    'Tenure': [8],\n",
        "    'MonthlyCharges': [105.00],\n",
        "    'TotalCharges': [840.00],\n",
        "    'ContractType': ['Month-to-month'],\n",
        "    'InternetService': ['Fiber optic'],\n",
        "    'PaymentMethod': ['Electronic check']\n",
        "})\n",
        "\n",
        "# Preprocess the new data (must match the training data preprocessing)\n",
        "# Need to apply the same one-hot encoding and ensure column order\n",
        "new_customer_encoded_example = pd.get_dummies(new_customer_data_example,\n",
        "                                                columns=['Gender', 'ContractType', 'InternetService', 'PaymentMethod'],\n",
        "                                                drop_first=True)\n",
        "\n",
        "# Ensure columns match the training data (add missing columns with 0)\n",
        "missing_cols_example = set(X_train_large.columns) - set(new_customer_encoded_example.columns)\n",
        "for c in missing_cols_example:\n",
        "    new_customer_encoded_example[c] = 0\n",
        "\n",
        "# Ensure the order of columns is the same\n",
        "new_customer_encoded_example = new_customer_encoded_example[X_train_large.columns]\n",
        "\n",
        "# Make prediction using the trained large model\n",
        "new_customer_prediction_large = rf_model_large.predict(new_customer_encoded_example)\n",
        "new_customer_churn_proba_large = rf_model_large.predict_proba(new_customer_encoded_example)[:, 1]\n",
        "\n",
        "print(f\"\\nPrediction for example new customer: {'Churn' if new_customer_prediction_large[0] == 1 else 'No Churn'}\")\n",
        "print(f\"Probability of Churn: {new_customer_churn_proba_large[0]:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yLl0GaxJ-QP",
        "outputId": "48718767-bc5d-4aef-e7b1-6e1ba68705fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated dataset with 10000 rows.\n",
            "Dataset head:\n",
            "   CustomerID  Age  Gender  Tenure  MonthlyCharges  TotalCharges  \\\n",
            "0           1   56  Female       3           16.16       1005.13   \n",
            "1           2   69    Male       1           64.93       5252.00   \n",
            "2           3   46    Male       0           50.29       3918.32   \n",
            "3           4   32  Female      64           85.86       6078.74   \n",
            "4           5   60    Male      45           82.03       3434.14   \n",
            "\n",
            "     ContractType InternetService              PaymentMethod  Churn  \n",
            "0        Two year              No           Electronic check      0  \n",
            "1        Two year     Fiber optic  Bank transfer (automatic)      0  \n",
            "2  Month-to-month     Fiber optic  Bank transfer (automatic)      0  \n",
            "3  Month-to-month             DSL           Electronic check      1  \n",
            "4        Two year             DSL    Credit card (automatic)      0  \n",
            "\n",
            "Churn Distribution:\n",
            "Churn\n",
            "0    0.6653\n",
            "1    0.3347\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Training set size: 7500\n",
            "Testing set size: 2500\n",
            "\n",
            "Training Random Forest model...\n",
            "Model training complete.\n",
            "\n",
            "Evaluating model performance...\n",
            "Model Evaluation on Large Dataset:\n",
            "Accuracy: 0.6652\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.90      0.78      1663\n",
            "           1       0.50      0.19      0.28       837\n",
            "\n",
            "    accuracy                           0.67      2500\n",
            "   macro avg       0.59      0.55      0.53      2500\n",
            "weighted avg       0.63      0.67      0.61      2500\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1503  160]\n",
            " [ 677  160]]\n",
            "\n",
            "ROC AUC Score: 0.6324\n",
            "\n",
            "Feature Importance (Top 10):\n",
            "                           feature  importance\n",
            "0                              Age    0.233217\n",
            "2                   MonthlyCharges    0.219065\n",
            "3                     TotalCharges    0.214809\n",
            "1                           Tenure    0.186207\n",
            "4                      Gender_Male    0.024272\n",
            "7      InternetService_Fiber optic    0.020294\n",
            "6            ContractType_Two year    0.019343\n",
            "10  PaymentMethod_Electronic check    0.017543\n",
            "5            ContractType_One year    0.017070\n",
            "11      PaymentMethod_Mailed check    0.016636\n",
            "\n",
            "Prediction for example new customer: Churn\n",
            "Probability of Churn: 0.5400\n"
          ]
        }
      ]
    }
  ]
}